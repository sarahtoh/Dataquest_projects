{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a Hacker News pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use comes from a [Hacker News](https://news.ycombinator.com/) (HN) API that returns JSON data of the top stories in 2014. \n",
    "\n",
    "The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "- created_at: A timestamp of the story's creation time.\n",
    "- created_at_i: A unix epoch timestamp.\n",
    "- url: The URL of the story link.\n",
    "- objectID: The ID of the story.\n",
    "- author: The story's author (username on HN).\n",
    "- points: The number of upvotes the story had.\n",
    "- title: The headline of the post.\n",
    "- num_comments: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
    "\n",
    "The pipeline will do the following:\n",
    "1. Read the data\n",
    "2. Filter popular stories (links (not Ask HN posts), have a good number of points, and have some comments)\n",
    "3. Write popular stories to a CSV file\n",
    "4. Extracting titles and cleaning them\n",
    "5. Getting word frequencies and sorting the frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and starting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import io\n",
    "import csv\n",
    "import string\n",
    "import itertools \n",
    "\n",
    "from pipeline import build_csv, Pipeline\n",
    "from stop_words import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "        \n",
    "# Getting the most popular stories\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return (story['points'] > 50) and (story['num_comments'] > 1) and not (story['title'].startswith('Ask HN'))\n",
    "    \n",
    "    return (story for story in stories if is_popular(story))\n",
    "\n",
    "# Writing popular stories to a CSV file\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(filtered_stories):\n",
    "    lines = []\n",
    "    for story in filtered_stories:\n",
    "        lines.append((story['objectID'], \n",
    "                      datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "                      story['url'], story['points'], story['title']))\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())\n",
    "\n",
    "# Extracting titles\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    return (line[idx] for line in reader)\n",
    "\n",
    "# Cleaning titles\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title\n",
    "\n",
    "# Calculating word frequencies\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(cleaned_titles):\n",
    "    word_freq = {}\n",
    "    for title in cleaned_titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq\n",
    "\n",
    "# Sorting word frequencies in descending order\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def most_frequent(keyword_dict):\n",
    "    sorted_dict ={k: v for k, v in sorted(keyword_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    sorted_100 = dict(itertools.islice(sorted_dict.items(), 100)) \n",
    "    top_100 = []\n",
    "    for key, val in sorted_100.items():\n",
    "        top_100.append((key,val))\n",
    "    return top_100\n",
    "\n",
    "#     alternative\n",
    "#     freq_tuple = [\n",
    "#         (word, word_freq[word])\n",
    "#         for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "#     ]\n",
    "#     return freq_tuple[:100]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "ran = pipeline.run()\n",
    "print(ran[most_frequent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible next steps:\n",
    "\n",
    "- Rewrite the Pipeline class' output to save a file of the output for each task. This will allow you to \"checkpoint\" tasks so they don't have to be run twice.\n",
    "- Use the nltk package for more advanced natural language processing tasks.\n",
    "- Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file.\n",
    "- Fetch the data from Hacker News directly from a JSON API. Instead of reading from the file we gave, you can perform additional data processing using newer data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
